{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 13:34:26.226205: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-27 13:34:26.226849: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-27 13:34:26.230257: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-27 13:34:26.241286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730050466.261056  786905 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730050466.265933  786905 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-27 13:34:26.284786: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import tensorflow as tf\n",
    "# Disable GPU and suppress TensorFlow messages\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "from tensorflow.keras.models import Model  # type: ignore\n",
    "from tensorflow.keras.regularizers import l1_l2  # type: ignore\n",
    "from tensorflow.keras.layers import (  # type: ignore\n",
    "    Input,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LayerNormalization,\n",
    "    MultiHeadAttention,\n",
    "    Add,\n",
    "    Lambda,\n",
    "    Layer,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Reshape,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam  # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint  # type: ignore\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import ta\n",
    "import pandas as pd\n",
    "\n",
    "COMPLETE_TRAINING = False\n",
    "STARTING_LR = 0.001\n",
    "# Add your project path\n",
    "sys.path.append(\n",
    "    \"/home/hamza-berrada/Desktop/cooding/airflow/airflow/pluggings/Live-Tools-V2\"\n",
    ")\n",
    "from utilities.bitget_perp import PerpBitget\n",
    "from strategies.nvp.embadding import generate_feature_vectors\n",
    "checkpoint_path = \"transformer_model_checkpoint.keras\"\n",
    "scaler_path = \"scaler.save\"\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# Register the custom layer with Keras serialization\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, maxlen, embed_dim, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_encoding = self.positional_encoding(maxlen, embed_dim)\n",
    "\n",
    "    def get_angles(self, pos, i, embed_dim):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, embed_dim):\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(position)[:, np.newaxis],\n",
    "            np.arange(embed_dim)[np.newaxis, :],\n",
    "            embed_dim,\n",
    "        )\n",
    "\n",
    "        # Apply sin to even indices (2i)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # Apply cos to odd indices (2i+1)\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        # Return the configuration of the layer, including maxlen and embed_dim\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Recreate the layer from the configuration\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "# Data Augmentation Function\n",
    "def augment_data(x, y, noise_factor=0.01):\n",
    "    x_noisy = x + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x.shape)\n",
    "    y_noisy = y + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=y.shape)\n",
    "    return x_noisy, y_noisy\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        # Gradually increase the learning rate for the first 10 epochs\n",
    "        return float(lr * 0.9)\n",
    "    else:\n",
    "        # Continue with the current learning rate\n",
    "        return float(lr)\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "# Generate Additional Features\n",
    "def generate_additional_features(df):\n",
    "    # Add time-based features\n",
    "    df[\"hour\"] = df.index.hour\n",
    "    df[\"day_of_week\"] = df.index.dt.dayofweek\n",
    "    df[\"is_weekend\"] = df.index.apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    # Add additional technical indicators\n",
    "    df[\"macd\"] = ta.trend.macd_diff(df[\"close\"])\n",
    "    df[\"stochastic\"] = ta.momentum.stoch(df[\"high\"], df[\"low\"], df[\"close\"])\n",
    "    bollinger = ta.volatility.BollingerBands(df[\"close\"])\n",
    "    df[\"bollinger_mavg\"] = bollinger.bollinger_mavg()\n",
    "    df[\"bollinger_hband\"] = bollinger.bollinger_hband()\n",
    "    df[\"bollinger_lband\"] = bollinger.bollinger_lband()\n",
    "\n",
    "    # Fill NaNs\n",
    "    df.bfill(inplace=True)\n",
    "    df.ffill(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Initialize the exchange\n",
    "    exchange = PerpBitget()\n",
    "    await exchange.load_markets()\n",
    "\n",
    "    # Select a single pair\n",
    "    pair = \"BTC/USDT\"\n",
    "\n",
    "    # Fetch OHLCV data\n",
    "    timeframe = \"1m\"\n",
    "    limit = 10000\n",
    "    df = await exchange.get_last_ohlcv(pair, timeframe, limit)\n",
    "\n",
    "    # Close the exchange session\n",
    "    await exchange.close()\n",
    "    # Check if 'timestamp' column exists, create one if not\n",
    "    if \"timestamp\" not in df.columns:\n",
    "        # Create a timestamp starting from current time, decrementing by 1 hour\n",
    "        now = datetime.datetime.now()\n",
    "        timestamps = [\n",
    "            now - datetime.timedelta(minutes=(len(df) - i - 1)) for i in range(len(df))\n",
    "        ]\n",
    "        df[\"timestamp\"] = timestamps\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "        \n",
    "    df.index = df[\"timestamp\"]\n",
    "    # Generate feature vectors\n",
    "    feature_vectors, feature_columns = generate_feature_vectors(df)\n",
    "\n",
    "    # Generate additional features\n",
    "    df = generate_additional_features(df)\n",
    "    additional_features = [\n",
    "        \"hour\",\n",
    "        \"day_of_week\",\n",
    "        \"is_weekend\",\n",
    "        \"macd\",\n",
    "        \"stochastic\",\n",
    "        \"bollinger_mavg\",\n",
    "        \"bollinger_hband\",\n",
    "        \"bollinger_lband\",\n",
    "    ]\n",
    "    additional_feature_vectors = df[additional_features].values\n",
    "\n",
    "    # Combine original and additional features\n",
    "    feature_vectors = np.concatenate(\n",
    "        (feature_vectors, additional_feature_vectors), axis=1\n",
    "    )\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    feature_vectors_scaled = scaler.fit_transform(feature_vectors)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    # Create sequences of feature vectors\n",
    "    sequence_length = 500\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(feature_vectors_scaled) - sequence_length - 20 + 1):\n",
    "        x.append(feature_vectors_scaled[i : i + sequence_length])\n",
    "        y.append(\n",
    "            feature_vectors_scaled[i + sequence_length : i + sequence_length + 20]\n",
    "        )  # Next 5 data points\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Data Augmentation\n",
    "    x_aug, y_aug = x,y#augment_data(x, y)\n",
    "\n",
    "    # Split data into training, validation, and testing sets\n",
    "    x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "        x_aug, y_aug, test_size=0.2, random_state=42\n",
    "    )\n",
    "    x_val, x_test, y_val, y_test = train_test_split(\n",
    "        x_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    # Define transformer model parameters\n",
    "    input_shape = (sequence_length, x.shape[2])\n",
    "    num_heads = 20\n",
    "    ff_dim = 128\n",
    "    num_transformer_blocks = 16\n",
    "    dropout_rate = 0.2\n",
    "    embedding_dim = 16  # Dimension of the in-model embedding\n",
    "    output_steps = 20  # Predict next 5 steps\n",
    "\n",
    "    # Build the transformer model\n",
    "    if COMPLETE_TRAINING and os.path.exists(checkpoint_path):\n",
    "        print(\"Loading model from checkpoint...\")\n",
    "        model = load_model(checkpoint_path)\n",
    "        scaler = joblib.load(scaler_path)\n",
    "    else:\n",
    "        print(\"Building a new model...\")\n",
    "        model = build_enhanced_transformer_model(\n",
    "            input_shape=input_shape,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=ff_dim,\n",
    "            num_transformer_blocks=num_transformer_blocks,\n",
    "            dropout_rate=dropout_rate,\n",
    "            embedding_dim=embedding_dim,\n",
    "            output_steps=output_steps,\n",
    "        )\n",
    "\n",
    "    # Train the transformer model\n",
    "    history = train_transformer_model(model, x_train, y_train, x_val, y_val)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, x_test, y_test)\n",
    "\n",
    "    # Save the transformer model and scaler\n",
    "    model.save(checkpoint_path)\n",
    "\n",
    "\n",
    "def build_enhanced_transformer_model(\n",
    "    input_shape,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    dropout_rate,\n",
    "    embedding_dim,\n",
    "    output_steps=1,\n",
    "):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Embedding layer with increased dimensions\n",
    "    embedding_layer = Dense(embedding_dim, activation=\"relu\")(inputs)\n",
    "\n",
    "    # Add positional encoding\n",
    "    x = PositionalEncoding(input_shape[0], embedding_dim)(embedding_layer)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # Multi-head attention\n",
    "        attn_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embedding_dim, dropout=dropout_rate\n",
    "        )(x, x)\n",
    "        x = Add()([x, attn_output])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = Dense(\n",
    "            ff_dim, activation=\"relu\", kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "        )(x)\n",
    "        ffn_output = BatchNormalization()(ffn_output)\n",
    "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "        x = Add()([x, ffn_output])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Output layer for multi-step forecasting\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(output_steps * input_shape[-1])(x)\n",
    "    outputs = Reshape((output_steps, input_shape[-1]))(outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=STARTING_LR), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_transformer_model(\n",
    "    model, x_train, y_train, x_val, y_val, epochs=100, batch_size=64\n",
    "):\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        checkpoint_path, monitor=\"val_loss\", save_best_only=True, verbose=1\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-6, verbose=1\n",
    "    )\n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[reduce_lr, early_stopping, lr_scheduler, checkpoint],\n",
    "        verbose=1,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    loss, mae = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = PerpBitget()\n",
    "await exchange.load_markets()\n",
    "\n",
    "# Select a single pair\n",
    "pair = \"BTC/USDT\"\n",
    "\n",
    "# Fetch OHLCV data\n",
    "timeframe = \"1m\"\n",
    "limit = 1000\n",
    "df = await exchange.get_last_ohlcv(pair, timeframe, limit)\n",
    "\n",
    "# Close the exchange session\n",
    "await exchange.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2024-10-27 02:35:00', '2024-10-27 02:36:00',\n",
      "               '2024-10-27 02:37:00', '2024-10-27 02:38:00',\n",
      "               '2024-10-27 02:39:00', '2024-10-27 02:40:00',\n",
      "               '2024-10-27 02:41:00', '2024-10-27 02:42:00',\n",
      "               '2024-10-27 02:43:00', '2024-10-27 02:44:00',\n",
      "               ...\n",
      "               '2024-10-27 17:25:00', '2024-10-27 17:26:00',\n",
      "               '2024-10-27 17:27:00', '2024-10-27 17:28:00',\n",
      "               '2024-10-27 17:29:00', '2024-10-27 17:30:00',\n",
      "               '2024-10-27 17:31:00', '2024-10-27 17:32:00',\n",
      "               '2024-10-27 17:33:00', '2024-10-27 17:34:00'],\n",
      "              dtype='datetime64[ns]', name='date', length=500, freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Date into data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index, unit=\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors, feature_columns = generate_feature_vectors(df)\n",
    "\n",
    "# Generate additional features\n",
    "# df = generate_additional_features(df)\n",
    "# additional_features = [\n",
    "#     \"hour\",\n",
    "#     \"day_of_week\",\n",
    "#     \"is_weekend\",\n",
    "#     \"macd\",\n",
    "#     \"stochastic\",\n",
    "#     \"bollinger_mavg\",\n",
    "#     \"bollinger_hband\",\n",
    "#     \"bollinger_lband\",\n",
    "# ]\n",
    "# additional_feature_vectors = df[additional_features].values\n",
    "\n",
    "# # Combine original and additional features\n",
    "# feature_vectors = np.concatenate(\n",
    "#     (feature_vectors, additional_feature_vectors), axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.70103000e+04  2.58000000e+01  2.57000000e+01  1.00000000e+00\n",
      "   6.70002000e+04  3.82592788e+01]\n",
      " [ 6.70360000e+04  1.80000000e+01  1.79000000e+01  1.00000000e+00\n",
      "   6.70002000e+04  3.82592788e+01]\n",
      " [ 6.70539000e+04  1.53000000e+01 -1.38000000e+01  0.00000000e+00\n",
      "   6.70002000e+04  3.82592788e+01]\n",
      " ...\n",
      " [ 6.79652000e+04  5.68000000e+01 -3.76000000e+01  0.00000000e+00\n",
      "   6.77200000e+04  7.13628545e+01]\n",
      " [ 6.79652000e+04  5.68000000e+01 -3.76000000e+01  0.00000000e+00\n",
      "   6.77200000e+04  7.13628545e+01]\n",
      " [ 6.79652000e+04  5.68000000e+01 -3.76000000e+01  0.00000000e+00\n",
      "   6.77200000e+04  7.13628545e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your numpy ndarray\n",
    "# Step 1: Convert numpy array to a DataFrame\n",
    "feature_vectors = pd.DataFrame(feature_vectors)\n",
    "# Step 2: Compute the correlation matrix\n",
    "correlation_matrix = feature_vectors.corr().abs()  # Absolute correlation values\n",
    "\n",
    "# Step 3: Identify and remove one of each pair of correlated features\n",
    "threshold = 0.9\n",
    "# Create a set to hold the columns to be dropped\n",
    "to_drop = set()\n",
    "\n",
    "# Iterate over the correlation matrix to find pairs above the threshold\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "        if correlation_matrix.iloc[i, j] > threshold:\n",
    "            # Get the column names\n",
    "            col1 = correlation_matrix.columns[i]\n",
    "            col2 = correlation_matrix.columns[j]\n",
    "            # Add one of the columns to the 'to_drop' set\n",
    "            to_drop.add(col2)\n",
    "\n",
    "# Drop the selected columns from the DataFrame\n",
    "df_reduced = feature_vectors.drop(columns=to_drop)\n",
    "\n",
    "# 'df_reduced' is the DataFrame with one feature removed from each correlated pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smoothed = df_reduced.rolling(window=3).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center and reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.save']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "feature_vectors_scaled = scaler.fit_transform(feature_vectors)\n",
    "joblib.dump(scaler, scaler_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a new model...\n",
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34s/step - loss: 78.7213 - mae: 5.8142 \n",
      "Epoch 1: val_loss improved from inf to 27.20676, saving model to transformer_model_checkpoint.keras\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 36s/step - loss: 77.5174 - mae: 5.8529 - val_loss: 27.2068 - val_mae: 4.4185 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 31s/step - loss: 26.7857 - mae: 4.4917"
     ]
    }
   ],
   "source": [
    "sequence_length = 500\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(feature_vectors_scaled) - sequence_length - 20 + 1):\n",
    "    x.append(feature_vectors_scaled[i : i + sequence_length])\n",
    "    y.append(\n",
    "        feature_vectors_scaled[i + sequence_length : i + sequence_length + 20]\n",
    "    )  # Next 5 data points\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "# Data Augmentation\n",
    "x_aug, y_aug = x,y#augment_data(x, y)\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x_aug, y_aug, test_size=0.2, random_state=42\n",
    ")\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Define transformer model parameters\n",
    "input_shape = (sequence_length, x.shape[2])\n",
    "num_heads = 8\n",
    "ff_dim = 128\n",
    "num_transformer_blocks = 8\n",
    "dropout_rate = 0.2\n",
    "embedding_dim = 128  # Dimension of the in-model embedding\n",
    "output_steps = 20  # Predict next 5 steps\n",
    "\n",
    "# Build the transformer model\n",
    "if COMPLETE_TRAINING and os.path.exists(checkpoint_path):\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = load_model(checkpoint_path,custom_objects={'PositionalEncoding': PositionalEncoding})\n",
    "    scaler = joblib.load(scaler_path)\n",
    "else:\n",
    "    print(\"Building a new model...\")\n",
    "    model = build_enhanced_transformer_model(\n",
    "        input_shape=input_shape,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        num_transformer_blocks=num_transformer_blocks,\n",
    "        dropout_rate=dropout_rate,\n",
    "        embedding_dim=embedding_dim,\n",
    "        output_steps=output_steps,\n",
    "    )\n",
    "\n",
    "# Train the transformer model\n",
    "history = train_transformer_model(model, x_train, y_train, x_val, y_val)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, x_test, y_test)\n",
    "\n",
    "# Save the transformer model and scaler\n",
    "model.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
